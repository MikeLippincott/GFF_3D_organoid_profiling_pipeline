{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import sqlite3\n",
    "from contextlib import closing\n",
    "from functools import reduce\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    cfg = get_ipython().config\n",
    "    in_notebook = True\n",
    "except NameError:\n",
    "    in_notebook = False\n",
    "\n",
    "    # Get the current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "if (cwd / \".git\").is_dir():\n",
    "    root_dir = cwd\n",
    "\n",
    "else:\n",
    "    root_dir = None\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \".git\").is_dir():\n",
    "            root_dir = parent\n",
    "            break\n",
    "\n",
    "# Check if a Git root directory was found\n",
    "if root_dir is None:\n",
    "    raise FileNotFoundError(\"No Git root directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 parquet files found\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook:\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument(\n",
    "        \"--well_fov\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Well and field of view to process, e.g. 'A01_1'\",\n",
    "    )\n",
    "    argparser.add_argument(\n",
    "        \"--patient\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Patient ID to process, e.g. 'P01'\",\n",
    "    )\n",
    "    args = argparser.parse_args()\n",
    "    well_fov = args.well_fov\n",
    "    patient = args.patient\n",
    "else:\n",
    "    well_fov = \"C4-2\"\n",
    "    patient = \"NF0014\"\n",
    "\n",
    "\n",
    "result_path = pathlib.Path(\n",
    "    f\"{root_dir}/data/{patient}/extracted_features/{well_fov}\"\n",
    ").resolve(strict=True)\n",
    "# schema save path\n",
    "schema_path = pathlib.Path(\n",
    "    f\"{root_dir}/4.processing_image_based_profiles/data/schemas/schema_db.duckdb\"\n",
    ").resolve()\n",
    "schema_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# get a list of all parquets in the directory recursively\n",
    "parquet_files = list(result_path.rglob(\"*.parquet\"))\n",
    "parquet_files.sort()\n",
    "print(len(parquet_files), \"parquet files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nested dictionary to hold the feature types and compartments\n",
    "feature_types = [\n",
    "    \"AreaSizeShape\",\n",
    "    \"Colocalization\",\n",
    "    \"Intensity\",\n",
    "    \"Granularity\",\n",
    "    \"Neighbor\",\n",
    "    \"Texture\",\n",
    "]\n",
    "compartments = [\"Organoid\", \"Nuclei\", \"Cell\", \"Cytoplasm\"]\n",
    "\n",
    "feature_types_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
    "# copy the feature types dictionary to another blank dictionary that will hold the parquet files\n",
    "\n",
    "merged_df_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
    "\n",
    "\n",
    "for file in parquet_files:\n",
    "    for compartment in feature_types_dict.keys():\n",
    "        for feature_type in feature_types_dict[compartment].keys():\n",
    "            if compartment in str(file) and feature_type in str(file):\n",
    "                feature_types_dict[compartment][feature_type].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compartment in feature_types_dict.keys():\n",
    "    for feature_type in feature_types_dict[compartment].keys():\n",
    "        if len(feature_types_dict[compartment][feature_type]) > 0:\n",
    "            for file in feature_types_dict[compartment][feature_type]:\n",
    "                # check if the file exists\n",
    "                if not file.exists():\n",
    "                    if (\n",
    "                        \"neighbor\" in file.name.lower()\n",
    "                        and \"nuclei\" not in file.name.lower()\n",
    "                    ):\n",
    "                        print(f\"File {file} does not exist\")\n",
    "                        continue\n",
    "                # check if the file is a parquet file\n",
    "                if not file.name.endswith(\".parquet\"):\n",
    "                    print(f\"File {file} is not a parquet file\")\n",
    "                    continue\n",
    "                # read the parquet files\n",
    "                try:\n",
    "                    df = duckdb.read_parquet(str(file)).to_df()\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error reading {feature_types_dict[compartment][feature_type]}: {e}\"\n",
    "                    )\n",
    "\n",
    "                # add the dataframe to the dictionary\n",
    "                merged_df_dict[compartment][feature_type].append(df)\n",
    "        else:\n",
    "            if (\n",
    "                \"neighbor\" in feature_type.lower()\n",
    "                and \"nuclei\" not in compartment.lower()\n",
    "            ):\n",
    "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
    "            else:\n",
    "                print(\n",
    "                    f\"No files found for {compartment} {feature_type}. Please check the directory.\"\n",
    "                )\n",
    "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
    "                for channel_df in merged_df_dict[compartment][feature_type]:\n",
    "                    if channel_df.empty:\n",
    "                        continue\n",
    "                    # check if the dataframe has the required columns\n",
    "                    if (\n",
    "                        \"object_id\" not in channel_df.columns\n",
    "                        or \"image_set\" not in channel_df.columns\n",
    "                    ):\n",
    "                        print(\n",
    "                            f\"Dataframe {channel_df} does not have the required columns\"\n",
    "                        )\n",
    "                        continue\n",
    "                    # check if the dataframe is empty\n",
    "                    if channel_df.empty:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = [\n",
    "    \"AreaSize_Shape\",\n",
    "    \"Colocalization\",\n",
    "    \"Intensity\",\n",
    "    \"Granularity\",\n",
    "    \"Neighbor\",\n",
    "    \"Texture\",\n",
    "]\n",
    "compartments = [\"Organoid\", \"Nuclei\", \"Cell\", \"Cytoplasm\"]\n",
    "\n",
    "final_df_dict = {\n",
    "    cmp: {ft: pd.DataFrame() for ft in feature_types} for cmp in compartments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compartment in merged_df_dict.keys():\n",
    "    for feature_type in merged_df_dict[compartment].keys():\n",
    "        for df in merged_df_dict[compartment][feature_type]:\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df.drop(columns=[\"__index_level_0__\"], inplace=True, errors=\"ignore\")\n",
    "            # if \"Texture\" not in feature_type:\n",
    "            final_df_dict[compartment][feature_type] = reduce(\n",
    "                lambda left, right: pd.merge(\n",
    "                    left, right, how=\"left\", on=[\"object_id\", \"image_set\"]\n",
    "                ),\n",
    "                merged_df_dict[compartment][feature_type],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(\n",
    "    {\n",
    "        \"object_id\": [],\n",
    "        \"image_set\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compartment_merged_dict = {\n",
    "    \"Organoid\": pd.DataFrame(),\n",
    "    \"Cell\": pd.DataFrame(),\n",
    "    \"Nuclei\": pd.DataFrame(),\n",
    "    \"Cytoplasm\": pd.DataFrame(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing compartment: Organoid\n",
      "Skipping Organoid Neighbor as it is not applicable for this compartment.\n",
      "Processing compartment: Nuclei\n",
      "Processing compartment: Cell\n",
      "Skipping Cell Neighbor as it is not applicable for this compartment.\n",
      "Processing compartment: Cytoplasm\n",
      "Skipping Cytoplasm Neighbor as it is not applicable for this compartment.\n"
     ]
    }
   ],
   "source": [
    "for compartment in final_df_dict.keys():\n",
    "    print(f\"Processing compartment: {compartment}\")\n",
    "    for feature_type in final_df_dict[compartment].keys():\n",
    "        if compartment != \"Nuclei\" and feature_type == \"Neighbor\":\n",
    "            print(\n",
    "                f\"Skipping {compartment} {feature_type} as it is not applicable for this compartment.\"\n",
    "            )\n",
    "            continue\n",
    "        if compartment_merged_dict[compartment].empty:\n",
    "            compartment_merged_dict[compartment] = final_df_dict[compartment][\n",
    "                feature_type\n",
    "            ].copy()\n",
    "        else:\n",
    "            compartment_merged_dict[compartment] = pd.merge(\n",
    "                compartment_merged_dict[compartment],\n",
    "                final_df_dict[compartment][feature_type],\n",
    "                on=[\"object_id\", \"image_set\"],\n",
    "                how=\"outer\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect(schema_path) as cx:\n",
    "    for compartment, df in compartment_merged_dict.items():\n",
    "        df = df.head(0)\n",
    "        cx.register(\"temp_df\", df)\n",
    "        cx.execute(f\"CREATE OR REPLACE TABLE {compartment} AS SELECT * FROM temp_df\")\n",
    "        cx.unregister(\"temp_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf1_image_based_profiling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
