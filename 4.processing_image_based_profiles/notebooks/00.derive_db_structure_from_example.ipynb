{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook and those following the \"DB_structure\" is a blank dataframe that is used to store the results of the profiling pipeline.\n",
    "This is used to insert blank dataframes into the final dataframe dictionary for each compartment and feature type if the record is empty so that a final df can be created and merged on the same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import sqlite3\n",
    "import sys\n",
    "from contextlib import closing\n",
    "from functools import reduce\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "if (cwd / \".git\").is_dir():\n",
    "    root_dir = cwd\n",
    "else:\n",
    "    root_dir = None\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \".git\").is_dir():\n",
    "            root_dir = parent\n",
    "            break\n",
    "sys.path.append(str(root_dir / \"utils\"))\n",
    "from notebook_init_utils import bandicoot_check, init_notebook\n",
    "from segmentation_init_utils import parse_segmentation_args\n",
    "\n",
    "root_dir, in_notebook = init_notebook()\n",
    "\n",
    "profile_base_dir = bandicoot_check(\n",
    "    pathlib.Path(\"/home/lippincm/mnt/bandicoot\").resolve(), root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 parquet files found\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook:\n",
    "    args = parse_segmentation_args()\n",
    "    well_fov = args[\"well_fov\"]\n",
    "    patient = args[\"patient\"]\n",
    "else:\n",
    "    well_fov = \"C4-2\"\n",
    "    patient = \"NF0014_T1\"\n",
    "\n",
    "\n",
    "result_path = pathlib.Path(\n",
    "    f\"{profile_base_dir}/data/{patient}/extracted_features/{well_fov}\"\n",
    ").resolve(strict=True)\n",
    "# DB_structure save path\n",
    "DB_structure_path = pathlib.Path(\n",
    "    f\"{root_dir}/4.processing_image_based_profiles/data/DB_structures/DB_structure_db.duckdb\"\n",
    ").resolve()\n",
    "DB_structure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# get a list of all parquets in the directory recursively\n",
    "parquet_files = list(result_path.rglob(\"*.parquet\"))\n",
    "parquet_files.sort()\n",
    "print(len(parquet_files), \"parquet files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nested dictionary to hold the feature types and compartments\n",
    "feature_types = [\n",
    "    \"AreaSizeShape\",\n",
    "    \"Colocalization\",\n",
    "    \"Intensity\",\n",
    "    \"Granularity\",\n",
    "    \"Neighbor\",\n",
    "    \"Texture\",\n",
    "]\n",
    "compartments = [\"Organoid\", \"Nuclei\", \"Cell\", \"Cytoplasm\"]\n",
    "\n",
    "feature_types_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
    "# copy the feature types dictionary to another blank dictionary that will hold the parquet files\n",
    "\n",
    "merged_df_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
    "\n",
    "\n",
    "for file in parquet_files:\n",
    "    [\n",
    "        feature_types_dict[compartment][feature_type].append(file)\n",
    "        for compartment in feature_types_dict.keys()\n",
    "        for feature_type in feature_types_dict[compartment].keys()\n",
    "        if compartment in str(file) and feature_type in str(file)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compartment in feature_types_dict.keys():\n",
    "    for feature_type in feature_types_dict[compartment].keys():\n",
    "        if len(feature_types_dict[compartment][feature_type]) > 0:\n",
    "            for file in feature_types_dict[compartment][feature_type]:\n",
    "                # check if the file exists\n",
    "                if not file.exists():\n",
    "                    if (\n",
    "                        \"neighbor\" in file.name.lower()\n",
    "                        and \"nuclei\" not in file.name.lower()\n",
    "                    ):\n",
    "                        print(f\"File {file} does not exist\")\n",
    "                        continue\n",
    "                # check if the file is a parquet file\n",
    "                if not file.name.endswith(\".parquet\"):\n",
    "                    print(f\"File {file} is not a parquet file\")\n",
    "                    continue\n",
    "                # read the parquet files\n",
    "                try:\n",
    "                    df = duckdb.read_parquet(str(file)).to_df()\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error reading {feature_types_dict[compartment][feature_type]}: {e}\"\n",
    "                    )\n",
    "\n",
    "                # add the dataframe to the dictionary\n",
    "                merged_df_dict[compartment][feature_type].append(df)\n",
    "        else:\n",
    "            if (\n",
    "                \"neighbor\" in feature_type.lower()\n",
    "                and \"nuclei\" not in compartment.lower()\n",
    "            ):\n",
    "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
    "            else:\n",
    "                print(\n",
    "                    f\"No files found for {compartment} {feature_type}. Please check the directory.\"\n",
    "                )\n",
    "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
    "                for channel_df in merged_df_dict[compartment][feature_type]:\n",
    "                    if channel_df.empty:\n",
    "                        continue\n",
    "                    # check if the dataframe has the required columns\n",
    "                    if (\n",
    "                        \"object_id\" not in channel_df.columns\n",
    "                        or \"image_set\" not in channel_df.columns\n",
    "                    ):\n",
    "                        print(\n",
    "                            f\"Dataframe {channel_df} does not have the required columns\"\n",
    "                        )\n",
    "                        continue\n",
    "                    # check if the dataframe is empty\n",
    "                    if channel_df.empty:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_dict = {\n",
    "    cmp: {ft: pd.DataFrame() for ft in feature_types} for cmp in compartments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the compartment, feature type, and the respective dataframes\n",
    "# merge the dataframes for each compartment and feature type on object id and image_set\n",
    "for compartment in merged_df_dict.keys():\n",
    "    for feature_type in merged_df_dict[compartment].keys():\n",
    "        for df in merged_df_dict[compartment][feature_type]:\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df.drop(columns=[\"__index_level_0__\"], inplace=True, errors=\"ignore\")\n",
    "            # if \"Texture\" not in feature_type:\n",
    "            final_df_dict[compartment][feature_type] = reduce(\n",
    "                lambda left, right: pd.merge(\n",
    "                    left, right, how=\"left\", on=[\"object_id\", \"image_set\"]\n",
    "                ),\n",
    "                merged_df_dict[compartment][feature_type],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(\n",
    "    {\n",
    "        \"object_id\": [],\n",
    "        \"image_set\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compartment_merged_dict = {\n",
    "    \"Organoid\": pd.DataFrame(),\n",
    "    \"Cell\": pd.DataFrame(),\n",
    "    \"Nuclei\": pd.DataFrame(),\n",
    "    \"Cytoplasm\": pd.DataFrame(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing compartment: Organoid\n",
      "Skipping Organoid Neighbor as it is not applicable for this compartment.\n",
      "Processing compartment: Nuclei\n",
      "Processing compartment: Cell\n",
      "Skipping Cell Neighbor as it is not applicable for this compartment.\n",
      "Processing compartment: Cytoplasm\n",
      "Skipping Cytoplasm Neighbor as it is not applicable for this compartment.\n"
     ]
    }
   ],
   "source": [
    "for compartment in final_df_dict.keys():\n",
    "    print(f\"Processing compartment: {compartment}\")\n",
    "    for feature_type in final_df_dict[compartment].keys():\n",
    "        # skip if the compartment is \"Nuclei\" and the feature type is \"Neighbor\"\n",
    "        if compartment != \"Nuclei\" and feature_type == \"Neighbor\":\n",
    "            print(\n",
    "                f\"Skipping {compartment} {feature_type} as it is not applicable for this compartment.\"\n",
    "            )\n",
    "            continue\n",
    "        # if the compartment df is empty then copy a blank dataframe in\n",
    "        if compartment_merged_dict[compartment].empty:\n",
    "            compartment_merged_dict[compartment] = final_df_dict[compartment][\n",
    "                feature_type\n",
    "            ].copy()\n",
    "        else:\n",
    "            compartment_merged_dict[compartment] = pd.merge(\n",
    "                compartment_merged_dict[compartment],\n",
    "                final_df_dict[compartment][feature_type],\n",
    "                on=[\"object_id\", \"image_set\"],\n",
    "                how=\"outer\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect(DB_structure_path) as cx:\n",
    "    for compartment, df in compartment_merged_dict.items():\n",
    "        df = df.head(0)\n",
    "        cx.register(\"temp_df\", df)\n",
    "        cx.execute(f\"CREATE OR REPLACE TABLE {compartment} AS SELECT * FROM temp_df\")\n",
    "        cx.unregister(\"temp_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf1_image_based_profiling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
