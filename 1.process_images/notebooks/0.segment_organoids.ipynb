{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-12-05 12:48:17.951500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "E0000 00:00:1733428097.984568  725133 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "E0000 00:00:1733428097.993662  725133 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2024-12-05 12:48:18.071129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                    ]
                }
            ],
            "source": [
                "from __future__ import absolute_import, division, print_function, unicode_literals\n",
                "\n",
                "import pathlib\n",
                "import sys\n",
                "\n",
                "import matplotlib\n",
                "import numpy as np\n",
                "\n",
                "matplotlib.rcParams[\"image.interpolation\"] = \"none\"\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'retina'\n",
                "\n",
                "from glob import glob\n",
                "\n",
                "import tifffile\n",
                "from csbdeep.io import save_tiff_imagej_compatible\n",
                "from csbdeep.utils import Path, normalize\n",
                "from stardist import random_label_cmap\n",
                "from stardist.models import StarDist3D\n",
                "from tifffile import imread\n",
                "\n",
                "np.random.seed(6)\n",
                "lbl_cmap = random_label_cmap()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(23, 1537, 1540)\n"
                    ]
                }
            ],
            "source": [
                "# set path to the test image\n",
                "image_file_path = pathlib.Path(\"../../data/z-stack_images/D3-1/D3-1_405.tif\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "\n",
                "nuclei_z_stack = tifffile.imread(image_file_path)\n",
                "print(nuclei_z_stack.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Onmipose"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-12-05 12:48:32,824\t[INFO ]\t[core.py    74   _use_gpu_torch    ]\t** TORCH GPU version installed and working. **\n",
                        ">>> GPU activated? 1\n"
                    ]
                }
            ],
            "source": [
                "# Import dependencies\n",
                "import numpy as np\n",
                "from cellpose_omni import core, models\n",
                "\n",
                "# This checks to see if you have set up your GPU properly.\n",
                "# CPU performance is a lot slower, but not a problem if you\n",
                "# are only processing a few images.\n",
                "use_GPU = core.use_gpu()\n",
                "print(\">>> GPU activated? %d\" % use_GPU)\n",
                "\n",
                "# for plotting\n",
                "import matplotlib as mpl\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "mpl.rcParams[\"figure.dpi\"] = 300\n",
                "plt.style.use(\"dark_background\")\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['/home/lippincm/Documents/GFF_3D_organoid_profiling_pipeline/1.process_images/test_dir/D3-1_405.tif']"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import pathlib\n",
                "\n",
                "import omnipose\n",
                "from cellpose_omni import io\n",
                "\n",
                "basedir = pathlib.Path(\"../test_dir/\").resolve(strict=True)\n",
                "files = io.get_image_files(str(basedir))\n",
                "files  # this displays the variable if it the last thing in the code block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original image shape: (23, 1537, 1540)\n",
                        "data type: uint16\n",
                        "data range: 0 65535\n",
                        "number of images: 1\n"
                    ]
                }
            ],
            "source": [
                "from cellpose_omni import io, transforms\n",
                "from omnipose.utils import normalize99\n",
                "\n",
                "imgs = [io.imread(f) for f in files]\n",
                "\n",
                "# print some info about the images.\n",
                "for i in imgs:\n",
                "    print(\"Original image shape:\", i.shape)\n",
                "    print(\"data type:\", i.dtype)\n",
                "    print(\"data range:\", i.min(), i.max())\n",
                "nimg = len(imgs)\n",
                "print(\"number of images:\", nimg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-12-05 12:48:39,033\t[INFO ]\t[models.py  427  __init__          ]\t>>plant_omni<< model set to be used\n",
                        "2024-12-05 12:48:39,034\t[INFO ]\t[core.py    88   assign_device     ]\t>>>> using CPU\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "resnet_torch.py (308): You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
                    ]
                }
            ],
            "source": [
                "from cellpose_omni import models\n",
                "\n",
                "model_name = \"plant_omni\"\n",
                "\n",
                "dim = 3\n",
                "nclasses = 3  # flow + dist + boundary\n",
                "nchan = 1\n",
                "omni = 1\n",
                "rescale = False\n",
                "diam_mean = 0\n",
                "use_GPU = 0  # Most people do not have enough VRAM to run on GPU... 24GB not enough for this image, need nearly 48GB\n",
                "model = models.CellposeModel(\n",
                "    gpu=use_GPU,\n",
                "    model_type=model_name,\n",
                "    net_avg=False,\n",
                "    diam_mean=diam_mean,\n",
                "    nclasses=nclasses,\n",
                "    dim=dim,\n",
                "    nchan=nchan,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-12-05 12:50:57,385\t[INFO ]\t[models.py  427  __init__          ]\t>>nuclei<< model set to be used\n",
                        "2024-12-05 12:50:57,399\t[INFO ]\t[core.py    74   _use_gpu_torch    ]\t** TORCH GPU version installed and working. **\n",
                        "2024-12-05 12:50:57,400\t[INFO ]\t[core.py    85   assign_device     ]\t>>>> using GPU\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "Error(s) in loading state_dict for CPnet:\n\tsize mismatch for downsample.down.res_down_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_1.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_2.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_3.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.proj.1.weight: copying a param with shape torch.Size([32, 2, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 2, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_1.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_2.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_3.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.proj.1.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_1.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_2.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_3.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.proj.1.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_1.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_2.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_3.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.proj.1.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.proj.1.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.proj.1.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.proj.1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.proj.1.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1, 1]).\n\tsize mismatch for output.2.weight: copying a param with shape torch.Size([3, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 32, 1, 1, 1]).\n\tsize mismatch for output.2.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([4]).",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m diam_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m use_GPU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Most people do not have enough VRAM to run on GPU... 24GB not enough for this image, need nearly 48GB\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCellposeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_GPU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_avg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdiam_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiam_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnchan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnchan\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniforge3/envs/GFF_cellpose/lib/python3.10/site-packages/cellpose_omni/models.py:496\u001b[0m, in \u001b[0;36mCellposeModel.__init__\u001b[0;34m(self, gpu, pretrained_model, model_type, net_avg, use_torch, diam_mean, device, residual_on, style_on, concatenation, nchan, nclasses, dim, omni, checkpoint, dropout, kernel_size)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model \u001b[38;5;241m=\u001b[39m pretrained_model\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    487\u001b[0m     \n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# dataparallel A1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# if self.torch and gpu:\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m#     self.net = nn.DataParallel(self.net)\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mcollect_params()\u001b[38;5;241m.\u001b[39mgrad_req \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
                        "File \u001b[0;32m~/miniforge3/envs/GFF_cellpose/lib/python3.10/site-packages/cellpose_omni/resnet_torch.py:295\u001b[0m, in \u001b[0;36mCPnet.load_model\u001b[0;34m(self, filename, cpu)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_GPU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbase,\n\u001b[1;32m    298\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnout,\n\u001b[1;32m    299\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msz,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_dropout,\n\u001b[1;32m    307\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size)\n",
                        "File \u001b[0;32m~/miniforge3/envs/GFF_cellpose/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CPnet:\n\tsize mismatch for downsample.down.res_down_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_1.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_2.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.conv.conv_3.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_0.proj.1.weight: copying a param with shape torch.Size([32, 2, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 2, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_1.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_2.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.conv.conv_3.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_1.proj.1.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_1.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_2.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.conv.conv_3.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_2.proj.1.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1, 1]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_1.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_2.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.conv.conv_3.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for downsample.down.res_down_3.proj.1.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_0.proj.1.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_1.proj.1.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_2.proj.1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1, 1]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for upsample.up.res_up_3.proj.1.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1, 1]).\n\tsize mismatch for output.2.weight: copying a param with shape torch.Size([3, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 32, 1, 1, 1]).\n\tsize mismatch for output.2.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([4])."
                    ]
                }
            ],
            "source": [
                "from cellpose_omni import models\n",
                "\n",
                "model_name = \"nuclei\"\n",
                "\n",
                "dim = 3\n",
                "nclasses = 3  # flow + dist + boundary\n",
                "nchan = 1\n",
                "omni = 1\n",
                "rescale = False\n",
                "diam_mean = 0\n",
                "use_GPU = True  # Most people do not have enough VRAM to run on GPU... 24GB not enough for this image, need nearly 48GB\n",
                "model = models.CellposeModel(\n",
                "    gpu=use_GPU,\n",
                "    model_type=model_name,\n",
                "    net_avg=False,\n",
                "    diam_mean=diam_mean,\n",
                "    nclasses=nclasses,\n",
                "    dim=dim,\n",
                "    nchan=nchan,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-12-05 10:39:19,759\t[WARNING]\t[models.py  699  eval              ]\tinput images must be a list of images, array of images, or dataloader\n",
                        "2024-12-05 10:39:19,760\t[INFO ]\t[models.py  709  eval              ]\tEvaluating with flow_threshold 0.00, mask_threshold -5.00\n",
                        "2024-12-05 10:39:19,760\t[INFO ]\t[models.py  711  eval              ]\tusing omni model, cluster False\n",
                        "2024-12-05 10:39:19,760\t[INFO ]\t[models.py  1097 eval              ]\tnot using dataparallel\n",
                        "failed to load model Error(s) in loading state_dict for CPnet:\n",
                        "\tsize mismatch for downsample.down.res_down_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_0.conv.conv_1.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_0.conv.conv_2.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_0.conv.conv_3.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_0.proj.1.weight: copying a param with shape torch.Size([32, 2, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 2, 1, 1, 1]).\n",
                        "\tsize mismatch for downsample.down.res_down_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_1.conv.conv_1.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_1.conv.conv_2.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_1.conv.conv_3.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_1.proj.1.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1, 1]).\n",
                        "\tsize mismatch for downsample.down.res_down_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_2.conv.conv_1.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_2.conv.conv_2.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_2.conv.conv_3.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_2.proj.1.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1, 1]).\n",
                        "\tsize mismatch for downsample.down.res_down_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_3.conv.conv_1.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_3.conv.conv_2.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_3.conv.conv_3.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for downsample.down.res_down_3.proj.1.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1, 1]).\n",
                        "\tsize mismatch for upsample.up.res_up_0.conv.conv_0.2.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_0.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_0.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_0.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_0.proj.1.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1, 1]).\n",
                        "\tsize mismatch for upsample.up.res_up_1.conv.conv_0.2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_1.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_1.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_1.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_1.proj.1.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1, 1]).\n",
                        "\tsize mismatch for upsample.up.res_up_2.conv.conv_0.2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_2.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_2.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_2.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_2.proj.1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1, 1]).\n",
                        "\tsize mismatch for upsample.up.res_up_3.conv.conv_0.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_3.conv.conv_1.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_3.conv.conv_2.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_3.conv.conv_3.conv.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n",
                        "\tsize mismatch for upsample.up.res_up_3.proj.1.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1, 1]).\n",
                        "\tsize mismatch for output.2.weight: copying a param with shape torch.Size([3, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 32, 1, 1, 1]).\n",
                        "\tsize mismatch for output.2.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([4]).\n",
                        "2024-12-05 10:39:19,841\t[INFO ]\t[models.py  1104 eval              ]\tshape before transforms.convert_image(): (23, 1537, 1540)\n",
                        "2024-12-05 10:39:19,842\t[INFO ]\t[transforms.py 506  convert_image     ]\tmulti-stack tiff read in as having 23 planes 1 channels\n",
                        "2024-12-05 10:39:19,916\t[INFO ]\t[models.py  1111 eval              ]\tshape after transforms.convert_image(): (23, 1537, 1540, 2)\n",
                        "2024-12-05 10:39:19,917\t[INFO ]\t[models.py  1115 eval              ]\tshape now (1, 23, 1537, 1540, 2)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "mask_threshold = -5  # usually this is -1\n",
                "flow_threshold = 0.0\n",
                "diam_threshold = 12\n",
                "net_avg = False\n",
                "cluster = False\n",
                "verbose = 1\n",
                "tile = 0\n",
                "chans = None\n",
                "compute_masks = 1\n",
                "resample = False\n",
                "rescale = None\n",
                "omni = True\n",
                "flow_factor = 10  # multiple to increase flow magnitude, useful in 3D\n",
                "transparency = True\n",
                "\n",
                "nimg = len(imgs)\n",
                "masks_om, flows_om = [[]] * nimg, [[]] * nimg\n",
                "\n",
                "# splitting the images into batches helps manage VRAM use so that memory can get properly released\n",
                "# here we have just one image, but most people will have several to process\n",
                "for k in range(nimg):\n",
                "    masks_om[k], flows_om[k], _ = model.eval(\n",
                "        imgs[k],\n",
                "        channels=chans,\n",
                "        rescale=rescale,\n",
                "        mask_threshold=mask_threshold,\n",
                "        net_avg=net_avg,\n",
                "        transparency=transparency,\n",
                "        flow_threshold=flow_threshold,\n",
                "        omni=omni,\n",
                "        resample=resample,\n",
                "        verbose=verbose,\n",
                "        diam_threshold=diam_threshold,\n",
                "        cluster=cluster,\n",
                "        niter=10,\n",
                "        tile=tile,\n",
                "        compute_masks=compute_masks,\n",
                "        flow_factor=flow_factor,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "import ncolor\n",
                "\n",
                "mask = masks_om[0]\n",
                "mask_nc = ncolor.label(mask, max_depth=20)\n",
                "\n",
                "import napari\n",
                "\n",
                "viewer = napari.view_labels(mask_nc)\n",
                "viewer.dims.ndisplay = 3\n",
                "viewer.camera.center = [s // 2 for s in mask.shape]\n",
                "viewer.camera.zoom = 1\n",
                "viewer.camera.angles = (10.90517458968619, -20.777067798396835, 58.04311170773853)\n",
                "viewer.camera.perspective = 0.0\n",
                "viewer.camera.interactive = True\n",
                "\n",
                "img = viewer.screenshot(size=(1000, 1000), scale=1, canvas_only=True, flash=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(3, 3), frameon=False)\n",
                "plt.imshow(img)\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from cellpose_omni import plot\n",
                "from omnipose.plot import apply_ncolor\n",
                "\n",
                "mu = flows_om[0][1]\n",
                "T = flows_om[0][2]\n",
                "bd = flows_om[0][4]\n",
                "# mu.shape,T.shape,bd.shape\n",
                "\n",
                "d = mu.shape[0]\n",
                "\n",
                "from omnipose.utils import rescale\n",
                "\n",
                "c = np.array([1] * 2 + [0] * (d - 2))\n",
                "# c = np.arange(d)\n",
                "\n",
                "\n",
                "def cyclic_perm(a):\n",
                "    n = len(a)\n",
                "    b = [[a[i - j] for i in range(n)] for j in range(n)]\n",
                "    return b\n",
                "\n",
                "\n",
                "slices = []\n",
                "idx = np.arange(d)\n",
                "cmap = mpl.colormaps[\"magma\"]\n",
                "cmap2 = mpl.colormaps[\"viridis\"]\n",
                "\n",
                "for inds in cyclic_perm(c):\n",
                "    slc = tuple([slice(-1) if i else mu.shape[k + 1] // 2 for i, k in zip(inds, idx)])\n",
                "    flow = plot.dx_to_circ(mu[np.where(inds) + slc], transparency=1) / 255\n",
                "    dist = cmap(rescale(T)[slc])\n",
                "    bnds = cmap2(rescale(bd)[slc])\n",
                "    msks = apply_ncolor(masks_om[0][slc])\n",
                "\n",
                "    fig = plt.figure(figsize=[5] * 2, frameon=False)\n",
                "    plt.imshow(np.hstack((flow, dist, bnds, msks)), interpolation=\"none\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from cellpose_omni import core, models\n",
                "\n",
                "# define cellpose model\n",
                "model_name = \"plant_cp\"\n",
                "\n",
                "# this model was trained on 2D slices\n",
                "dim = 2\n",
                "nclasses = 2  # cellpose models have no boundary field, just flow and distance\n",
                "\n",
                "# Cellpose defaults to 2 channels;\n",
                "# this is the setup for grayscale in that case\n",
                "nchan = 2\n",
                "chans = [0, 0]\n",
                "\n",
                "# no rescaling for this model\n",
                "diam_mean = 0\n",
                "\n",
                "\n",
                "use_GPU = core.use_gpu()\n",
                "model = models.CellposeModel(\n",
                "    gpu=use_GPU,\n",
                "    model_type=model_name,\n",
                "    net_avg=False,\n",
                "    diam_mean=diam_mean,\n",
                "    nclasses=nclasses,\n",
                "    dim=dim,\n",
                "    nchan=nchan,\n",
                ")\n",
                "\n",
                "\n",
                "# segmentation parameters\n",
                "omni = 1\n",
                "rescale = False\n",
                "mask_threshold = 0\n",
                "net_avg = 0\n",
                "verbose = 1\n",
                "tile = 0\n",
                "compute_masks = 1\n",
                "rescale = None\n",
                "flow_threshold = 0.0\n",
                "do_3D = True\n",
                "flow_factor = 10\n",
                "\n",
                "masks_cp, flows_cp, _ = model.eval(\n",
                "    imgs,\n",
                "    channels=chans,\n",
                "    rescale=rescale,\n",
                "    mask_threshold=mask_threshold,\n",
                "    net_avg=net_avg,\n",
                "    transparency=True,\n",
                "    flow_threshold=flow_threshold,\n",
                "    verbose=verbose,\n",
                "    tile=tile,\n",
                "    compute_masks=compute_masks,\n",
                "    do_3D=True,\n",
                "    omni=omni,\n",
                "    flow_factor=flow_factor,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "import ncolor\n",
                "\n",
                "mask = masks_cp[0]\n",
                "mask_nc = ncolor.label(mask, max_depth=20)\n",
                "\n",
                "import napari\n",
                "\n",
                "viewer = napari.view_labels(mask_nc)\n",
                "viewer.dims.ndisplay = 3\n",
                "viewer.camera.center = [s // 2 for s in mask.shape]\n",
                "viewer.camera.zoom = 1\n",
                "viewer.camera.angles = (10.90517458968619, -20.777067798396835, 58.04311170773853)\n",
                "viewer.camera.perspective = 0.0\n",
                "viewer.camera.interactive = True\n",
                "\n",
                "img = viewer.screenshot(size=(1000, 1000), scale=1, canvas_only=True, flash=False)\n",
                "plt.figure(figsize=(3, 3), frameon=False)\n",
                "plt.imshow(img)\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cellpose"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from cellpose_omni import core, models\n",
                "from skimage import io\n",
                "from skimage.exposure import rescale_intensity\n",
                "\n",
                "# Clear any previous sessions\n",
                "tf.keras.backend.clear_session()\n",
                "\n",
                "# Load your image data\n",
                "nuclei_z_stack\n",
                "\n",
                "# Check the shape of the array\n",
                "print(nuclei_z_stack.shape)\n",
                "\n",
                "# Normalize the image\n",
                "# Adjust the axis based on the shape of nuclei_z_stack\n",
                "# axis_norm = 0  # or 1, depending on the shape\n",
                "# img = rescale_intensity(nuclei_z_stack, in_range=(1, 99.8), out_range=(0, 1))\n",
                "img = nuclei_z_stack\n",
                "# Initialize the model\n",
                "model_name = \"nuclei\"\n",
                "use_GPU = core.use_gpu()\n",
                "model = models.CellposeModel(gpu=use_GPU, model_type=model_name)\n",
                "\n",
                "# Perform segmentation\n",
                "labels, details, _ = model.eval(img, diameter=50, channels=[0, 0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot the image and masks\n",
                "fig = plt.figure(figsize=(10, 5))\n",
                "plt.subplot(121)\n",
                "plt.imshow(nuclei_z_stack[8, :, :], cmap=\"gray\")\n",
                "plt.title(\"image\")\n",
                "plt.axis(\"off\")\n",
                "plt.subplot(122)\n",
                "plt.imshow(labels[8], cmap=\"magma\")\n",
                "plt.title(\"masks\")\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# show each z slice of the image and masks\n",
                "for z in range(nuclei_z_stack.shape[0]):\n",
                "    fig = plt.figure(figsize=(10, 5))\n",
                "    plt.subplot(121)\n",
                "    plt.imshow(nuclei_z_stack[z, :, :], cmap=\"gray\")\n",
                "    plt.title(\"image\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.subplot(122)\n",
                "    plt.imshow(labels[z], cmap=\"magma\")\n",
                "    plt.title(\"masks\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gff_preprocessing_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
